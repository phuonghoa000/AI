{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT05: Neural Net\n",
    "\n",
    "TODO: Võ Phương Hòa - 1412192\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng thể\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "Nên nhớ mục tiêu chính ở đây là *học, học một cách chân thật*. Bạn có thể thảo luận ý tưởng với bạn khác cũng như là tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn, dựa trên sự hiểu của bạn*. Nếu vi phạm thì sẽ bị 0 điểm cho toàn bộ môn học.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, bạn chọn `Kernel` -> `Restart & Run All` (restart python và chạy tất cả các cell), rồi kiểm tra xem có bị lỗi gì không.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file `BT05-NeuralNet.ipynb` (không cần nộp file dữ liệu); rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Trong bài này, bạn sẽ cài đặt mô hình Neural Net để phân lớp ảnh. Bộ dữ liệu được sử dụng là CIFAR-10. Đầu tiên, bạn đọc mô tả về bộ dữ liệu CIFAR-10 [ở đây](https://www.cs.toronto.edu/~kriz/cifar.html). Sau đó, bạn download file dữ liệu `CIFAR-10 python version`, giải nén ra được thư mục `cifar-10-batches-py`, và đặt thư mục này vào cùng thư mục chứa file notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "# You can also import other things ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Định nghĩa các hàm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm đọc bộ dữ liệu CIFAR-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "def read_data(containing_dir, num_train_batchs):\n",
    "    '''\n",
    "    Read training and test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    containing_dir : string\n",
    "        The directory containing data files.\n",
    "    num_train_batchs : int\n",
    "        The number of data batchs used for training \n",
    "        (there are totally 5 data batchs; each one contains 10000 images).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (train_X, train_Y, test_X, test Y) : tuple\n",
    "        train_X : numpy array, shape (num_train_batchs*10000, 32x32x3 + 1)\n",
    "            The matrix of input vectors (each row corresponds to an input vector) of the training set;\n",
    "            pixel values are normalized to [0, 1]; \n",
    "            the first column of this matrix is all ones (corresponding to x_0).\n",
    "        train_Y : numpy array, shape (num_train_batchs*10000, 1)\n",
    "            The vector of outputs of the training set.\n",
    "        test_X : numpy array, shape (10000, 32x32x3 + 1)\n",
    "            The matrix of input vectors of the test set (similar to train_X).\n",
    "        test_Y : numpy array, shape (10000, 1)\n",
    "            The vector of outputs of the test set.\n",
    "    '''\n",
    "    # Read training data\n",
    "    train_X_batchs = []\n",
    "    train_Y_batchs = []\n",
    "    for batch_idx in range(num_train_batchs):\n",
    "        batch = unpickle(containing_dir + '\\\\data_batch_' + str(batch_idx + 1))\n",
    "        train_X_batchs.append(batch['data'])\n",
    "        train_Y_batchs.append(np.array(batch['labels']).reshape(-1, 1))\n",
    "    train_X = np.vstack(train_X_batchs)\n",
    "    train_X = train_X / 255. # Normalize to [0, 1]\n",
    "    train_X = np.hstack([np.ones((len(train_X), 1)), train_X])\n",
    "    train_Y = np.vstack(train_Y_batchs)\n",
    "    \n",
    "    # Read test data\n",
    "    batch = unpickle(containing_dir + '\\\\test_batch')\n",
    "    test_X = batch['data']\n",
    "    test_X = test_X / 255. # Normalize to [0, 1]\n",
    "    test_X = np.hstack([np.ones((len(test_X), 1)), test_X])\n",
    "    test_Y = np.array(batch['labels']).reshape(-1, 1)\n",
    "    \n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm tính output của Neural Net**\n",
    "\n",
    "Ở đây, ta dùng hàm kích hoạt sigmoid ở các tầng ẩn, và hàm softmax ở tầng cuối."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(S):\n",
    "    '''\n",
    "    Computes sigmoid function for each element of array S.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-S))\n",
    "def softmax(S):\n",
    "    '''\n",
    "    Computes softmax function for each row of array S.\n",
    "    '''\n",
    "    A = np.exp(S)\n",
    "    A /= A.sum(axis=1, keepdims=True)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_nnet_outputs(Ws, X, need_all_layer_outputs):\n",
    "    '''\n",
    "    Computes the outputs of Neural Net by forward propagating X through the net.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Ws : list of numpy arrays\n",
    "        Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer; it doesn't have W);\n",
    "        W of layer l will have the shape of (d^(l-1)+1, d^(l)), where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1 and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    need_all_layer_outputs : bool\n",
    "        If this var is true, we'll return a list of layer's-outputs; \n",
    "        otherwise, we'll return the final layer's output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    If `need_all_layer_outputs` is true, return\n",
    "        A : numpy array, shape (N, K=10)\n",
    "            The maxtrix of output vectors of final layer; each row is an output vector (containing \n",
    "            each class's probability given the corresponding input vector).\n",
    "    Else, return\n",
    "        As : list of numpy arrays\n",
    "            As[l] is the matrix of output vectors of layer l; each row is an output vector (corresponding \n",
    "            to an input vector).\n",
    "    '''    \n",
    "    # TODO\n",
    "    As = [X]\n",
    "    for ws in Ws:\n",
    "        As.append(np.zeros(shape = (len(X), ws.shape[1])))\n",
    "    \n",
    "    for i in range(1, len(As) - 1):\n",
    "        As[i] = np.hstack([np.ones((len(X), 1)), sigmoid(np.dot(As[i - 1], Ws[i - 1]))])\n",
    "    As[-1] = softmax(np.dot(As[-2], Ws[-1]))\n",
    "    \n",
    "    if need_all_layer_outputs:\n",
    "        return As\n",
    "    \n",
    "    return As[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm huấn luyện Neural Net**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm `train_nnet` dưới đây đã được viết gần hết. Nhiệm vụ của bạn là:\n",
    "- *Đọc hiểu code*. Một số ghi chú:\n",
    "    - Trọng số nối từ tầng i đến tầng i+1 sẽ được khởi tạo ngẫu nhiên theo phân bố Gauss với mean bằng 0 và phương sai bằng *1/[(số nơ-ron tầng i) + 1]* (cộng 1 là tính thêm nơ-ron +1 ở tầng i). Tại sao lại dùng phương sai như vậy? Có thể mình sẽ nói cụ thể hơn ở trên lớp, nhưng ý ở đây là: xét một nơ-ron A ở tầng i+1, nếu số nơ-ron ở tầng i càng nhiều thì sẽ càng có nhiều trọng số đi vào nơ-ron A $\\rightarrow$ giá trị trước kích hoạt của nơ-ron A sẽ càng dễ rơi vào hai cực của hàm sigmoid (dẫn đến học chậm) $\\rightarrow$ cần giảm phương sai của phân bố Gauss lại khi số lượng nơ-ron ở tầng i tăng lên. \n",
    "    - Trong quá trình back-prop, ta không tính delta cho nơ-ron +1 (vì tính để làm gì?).\n",
    "- *Bổ sung 3 dòng code*:\n",
    "    - (1) Tính delta của tầng cuối, từ đó (2) tính gradient của W của tầng cuối (tính từ delta của tầng cuối và ouput của tầng kế cuối). Vì tầng cuối là tầng softmax (đã làm ở bài tập trước) nên bạn nên có đủ khả năng để viết được 2 dòng code này.\n",
    "    - (3) Tính gradient của W của tầng l (tính từ delta của tầng l và ouput của tầng l-1): tương tự (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nnet(X, Y, hid_layer_sizes, mb_size, learning_rate, max_epoch):\n",
    "    '''\n",
    "    Trains Neural Net on the dataset (X, Y).\n",
    "    Cost function: mean negative log likelihood.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD).\n",
    "    \n",
    "    Your code also needs to print out the cost and mean binary error on the training set after \n",
    "    each epoch (e.g., 'Epoch ..., cost ..., err ...%').\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "    hid_layer_sizes : list\n",
    "        The list of hidden layer sizes; e.g., hid_layer_sizes = [20, 10] means: the Net has 2 hidden \n",
    "        layers, the 1st one has 20 neurons, and the 2nd one has 10 neurons (not count the +1 neurons).\n",
    "    mb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    max_epoch : int\n",
    "        After this number of epochs, we'll terminate SGD.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (Ws, costs, errs) : tuple\n",
    "        Ws : list of numpy arrays\n",
    "            Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer; it doesn't have W);\n",
    "            W of layer l will have the shape of (d^(l-1)+1, d^(l)), where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1 and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "        costs : list, len = max_epoch\n",
    "            The list of costs after each epoch.\n",
    "        errs : list, len = max_epoch\n",
    "            The list of mean binary errors (on the training set) after each epoch.\n",
    "    '''\n",
    "    # Prepare for training\n",
    "    K = len(np.unique(Y)) # Num classes\n",
    "    layer_sizes = [X.shape[1] - 1] + hid_layer_sizes + [K]\n",
    "    np.random.seed(0) # This will fix the randomization; so, you and me will have the same results\n",
    "    Ws = [np.random.randn(layer_sizes[i] + 1, layer_sizes[i + 1]) / np.sqrt(layer_sizes[i] + 1) \n",
    "          for i in range(len(layer_sizes) - 1)] # Init Ws\n",
    "    one_hot_Y = np.zeros((len(Y), K))\n",
    "    one_hot_Y[np.arange(len(Y)), Y.reshape(-1)] = 1\n",
    "    costs = [] # To save costs during training\n",
    "    errs = [] # To save mean binary errors during training\n",
    "    N = len(X) # Num training examples\n",
    "    rnd_idxs = range(N) # Random indexes\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(max_epoch):\n",
    "        np.random.shuffle(rnd_idxs)\n",
    "        for start_idx in range(0, N, mb_size):\n",
    "            # Get minibach\n",
    "            mb_X = X[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            mb_Y = one_hot_Y[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            \n",
    "            # Forward-prop\n",
    "            As = compute_nnet_outputs(Ws, mb_X, True)\n",
    "         \n",
    "            # Back-prop; on the way, compute each layer's gradient and update its W\n",
    "            # TODO: delta = ...\n",
    "            delta = (As[-1] - mb_Y) / len(mb_Y)\n",
    "            # TODO: grad = ...\n",
    "            grad = np.dot(As[-2].T, delta)\n",
    "            Ws[-1] -= learning_rate * grad\n",
    "            for i in range(2, len(Ws) + 1):\n",
    "                delta = delta.dot(Ws[-i + 1].T[:, 1:]) * As[-i][:, 1:] * (1 - As[-i][:, 1:])\n",
    "                # TODO: grad = ...\n",
    "                grad = np.dot(As[-i - 1].T, delta)\n",
    "                Ws[-i] -= learning_rate * grad\n",
    "        \n",
    "        # Compute training info, save it, and print it\n",
    "        A = compute_nnet_outputs(Ws, X, False)\n",
    "        cost = np.mean(-np.sum(one_hot_Y * np.log(A), axis=1))\n",
    "        err = np.mean(np.argmax(A, axis=1) != Y.squeeze()) * 100\n",
    "        costs.append(cost)\n",
    "        errs.append(err)\n",
    "        print 'Epoch %d, cost %.3f, err %.3f%%' %(epoch, cost, err)\n",
    "            \n",
    "    return Ws, costs, errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chạy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Đọc dữ liệu (bạn cần đặt thư mục chứa dữ liệu `cifar-10-batches-py` vào cùng thư mục chứa file notebook này)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train_batchs = 3 # It's enough to make my computer tired\n",
    "train_X, train_Y, test_X, test_Y = read_data('cifar-10-batches-py', num_train_batchs)\n",
    "print 'train_X.shape = %s, train_Y.shape = %s' %(train_X.shape, train_Y.shape)\n",
    "print 'test_X.shape  = %s, test_Y.shape  = %s' %(test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Để thấy được sự ảnh hưởng của số lượng nơ-ron ẩn, ta sẽ lần lượt huấn luyện Neural Net với `hid_layer_sizes = [50]`, `[100]`, và `[200]` (cố định `mb_size = 32`, `learning_rate = 0.01`, `max_epoch = 200`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_0, costs_0, errs_0 = train_nnet(train_X, train_Y, [50], 32, 0.01, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy của mình ở 3 epoch đầu (để bạn có thể check nhanh là đã cài đúng hay chưa):\n",
    "\n",
    "`Epoch 0, cost 2.058, err 71.193%\n",
    "Epoch 1, cost 1.955, err 68.027%\n",
    "Epoch 2, cost 1.899, err 65.637%`\n",
    "\n",
    "và ở epoch cuối (chỉ số epoch tính từ 0): \n",
    "\n",
    "`Epoch 199, cost 1.139, err 39.657%` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_1, costs_1, errs_1 = train_nnet(train_X, train_Y, [100], 32, 0.01, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy của mình ở epoch cuối (chỉ số epoch tính từ 0): `Epoch 199, cost 1.036, err 35.687%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_2, costs_2, errs_2 = train_nnet(train_X, train_Y, [200], 32, 0.01, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy của mình ở epoch cuối (chỉ số epoch tính từ 0): `Epoch 199, cost 0.952, err 32.057%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "epochs = np.arange(len(costs_0))\n",
    "plt.plot(epochs, costs_0, label='50 hidden neurons')\n",
    "plt.plot(epochs, costs_1, label='100 hidden neurons')\n",
    "plt.plot(epochs, costs_2, label='200 hidden neurons')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cost')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bình luận về đồ thị kết quả: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Tăng tốc SGD bằng momentum (*đây là phần làm thêm, không bắt buộc*)\n",
    "\n",
    "Mã giả để cập nhật W của SGD-momentum như sau:\n",
    "\n",
    "`V = momentum_param*V - learning_rate*grad\n",
    "W = W + V`\n",
    "\n",
    "với:\n",
    "\n",
    "- `grad` là gradient trung bình trên minibatch\n",
    "- `V` là mảng có cùng shape với `W`, ban đầu có thể khởi tạo cho `V` là một mảng toàn 0\n",
    "- `momentum_param` là siêu tham số thuộc [0, 1)\n",
    "\n",
    "Như có thể thấy, bước đi tại thời điểm t hiện tại của SGD-momentum không chỉ dùng gradient (trung bình trên minibatch) tại thời điểm t hiện tại như SGD, mà còn dùng thêm gradient ở các thời điểm trước đó (t-1, t-2, t-3, ...) với trọng số giảm dần (`momentum_param`, `momentum_param` bình phương, `momentum_param` lập phương, ...). Bằng cách xem xét thêm các gradient trong quá khứ, momentum giúp tăng tốc SGD: những hướng đi nhất quán sẽ được tăng cường, những hướng đi không nhất quán sẽ được giảm bớt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nnet_1(X, Y, hid_layer_sizes, mb_size, learning_rate, max_epoch, momentum_param):\n",
    "    '''\n",
    "    Trains Neural Net on the dataset (X, Y).\n",
    "    Cost function: mean negative log likelihood.\n",
    "    Optimization algorithm: Stochastic Gradient Descent (SGD) + momentum.\n",
    "    \n",
    "    Your code also needs to print out the cost and mean binary error on the training set after \n",
    "    each epoch (e.g., 'Epoch ..., cost ..., err ...%').\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "    hid_layer_sizes : list\n",
    "        The list of hidden layer sizes; e.g., hid_layer_sizes = [20, 10] means: the Net has 2 hidden \n",
    "        layers, the 1st one has 20 neurons, and the 2nd one has 10 neurons (not count the +1 neurons).\n",
    "    mb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    max_epoch : int\n",
    "        After this number of epochs, we'll terminate SGD.\n",
    "    momentum_param : float, in [0, 1)\n",
    "        Momentum parameter; it determines how quickly the contributions of previous gradients decay.\n",
    "    Returns\n",
    "    -------\n",
    "    (Ws, costs, errs) : tuple\n",
    "        Ws : list of numpy arrays\n",
    "            Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer; it doesn't have W);\n",
    "            W of layer l will have the shape of (d^(l-1)+1, d^(l)), where \n",
    "            d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1 and \n",
    "            d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "        costs : list, len = max_epoch\n",
    "            The list of costs after each epoch.\n",
    "        errs : list, len = max_epoch\n",
    "            The list of mean binary errors (on the training set) after each epoch.\n",
    "    '''\n",
    "    # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W_3, costs_3, errs_3 = train_nnet_1(train_X, train_Y, [50], 32, 0.01, 200, momentum_param=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả chạy của mình ở epoch cuối (chỉ số epoch tính từ 0): `Epoch 199, cost 0.949, err 32.840%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize SGD and SGD-momentum\n",
    "epochs = np.arange(len(costs_0))\n",
    "plt.plot(epochs, costs_0, label='SGD')\n",
    "plt.plot(epochs, costs_3, label='SGD-momentum')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cost')\n",
    "plt.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "509px",
    "left": "0px",
    "right": "1212px",
    "top": "106px",
    "width": "68px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
